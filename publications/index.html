<!DOCTYPE html>
<html lang="en">
  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      Publications | Stella Grasshof
    
  
</title>
<meta name="author" content="Stella Grasshof">
<meta name="description" content="publications by categories in reversed chronological order. (generated by jekyll-scholar.)">

  <meta name="keywords" content="machine learning, academic, university">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04">
<!-- <link rel="stylesheet" href="/assets/css/mdb.min.css?62a43d1430ddb46fc4886f9d0e3b49b8"> -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5">
<link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap">

<!-- Code Syntax Highlighting -->
<link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light">



<!-- Styles -->

<link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e">
<link rel="canonical" href="http://localhost:4000/publications/">

<!-- Dark Mode -->

  <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark">
  <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="/">
          
            
              <span class="font-weight-bold">Stella</span>
            
            
            Grasshof
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">About
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/blog/">Blog
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item active">
                  
                  <a class="nav-link" href="/publications/">Publications
                    
                      <span class="sr-only">(current)</span>
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/studentprojects/">Studentprojects
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/teaching/">Teaching
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="/cv/">CV
                    
                  </a>
                </li>
              
            
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="fa-solid fa-moon"></i>
                <i class="fa-solid fa-sun"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        <div class="post">
  <header class="post-header">
    <h1 class="post-title">Publications</h1>
    <p class="post-description">publications by categories in reversed chronological order. (generated by jekyll-scholar.)</p>
  </header>

  <article>
    <!-- _pages/publications.md -->
<div class="publications">

<h2 class="bibliography">2024</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="marnet_bridging_2024" class="col-sm-8">
    <!-- Title -->
    <div class="title">Bridging the Sim-to-Real GAP for Underwater Image Segmentation</div>
    <!-- Author -->
    <div class="author">
      

      
      Luiza Ribeiro
            Marnet
          , <em>Stella
            Graßhof</em>, Yuri
            Brodskiy
          , and
        <span class="more-authors" title="click to view 1 more author" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '1 more author' ? 'Andrzej Wąsowski' : '1 more author';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.text(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">1 more author</span>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In Oceans</em> ,  May 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Labeling images for every new task or data pattern a model needs to learn is a significant time bottleneck in real-world applications. Moreover, acquiring the necessary data for training the models can be challenging. Ideally, one would train the models with simulated images and adapt them for the desired real tasks using the least possible amount of data. Active learning can be used to solve this problem with minimal effort. In this work, we train SegFormer for pipeline segmentation with synthetic images from an underwater simulated environment and fine-tune the model with real underwater pipeline images recorded in a marina. 
The evaluation shows that selecting real data with active learning for fine-tuning the model gives better results than randomly selecting the images. As part of the work, we release the dataset recorded in the marina, MarinaPipe, which will be publicly available.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="marnet_uncertainty_2024" class="col-sm-8">
    <!-- Title -->
    <div class="title">Uncertainty Driven Active Learning for Image Segmentation in Underwater Inspection</div>
    <!-- Author -->
    <div class="author">
      

      
      Luiza Ribeiro
            Marnet
          , <em>Stella
            Graßhof</em>, Yuri
            Brodskiy
          , and
        <span class="more-authors" title="click to view 1 more author" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '1 more author' ? 'Andrzej Wąsowski' : '1 more author';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.text(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">1 more author</span>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In ROBOVIS</em> ,  Mar 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Active learning aims to select the minimum amount of data to train a model that performs similarly to a model trained with the entire dataset. We study the potential of active learning for image segmentation in underwater infrastructure inspection tasks, where large amounts of data are typically collected. The pipeline inspection images are usually semantically repetitive but with great variations in quality. We use mutual information as the acquisition function, calculated using Monte Carlo dropout. HyperSeg is trained using active learning with an underwater pipeline inspection dataset of over 50,000 images. To allow reproducibility and assess the framework’s effectiveness, the CamVid dataset was also utilized. For the pipeline dataset, HyperSeg with active learning achieved 67.5% meanIoU using 12.5% of the data, and 61.4
% with the same amount of randomly selected images. This shows that using active learning for segmentation models in underwater inspection tasks can lower the cost significantly.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="haas_discovering_2024" class="col-sm-8">
    <!-- Title -->
    <div class="title">Discovering Interpretable Directions in the Semantic Latent Space of Diffusion Models</div>
    <!-- Author -->
    <div class="author">
      

      
      René
            Haas
          , Inbar
            Huberman-Spiegelglas
          , Rotem
            Mulayoff
          , and
        <span class="more-authors" title="click to view 3 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '3 more authors' ? 'Sami Brandt, Stella Graßhof, Tomer Michaeli' : '3 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.text(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">3 more authors</span>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 18th IEEE International Conference on Automatic Face and Gesture Recognition</em> ,  May 2024
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Denoising Diffusion Models (DDMs) have emerged as a strong competitor to Generative Adversarial Networks (GANs).
However, despite their widespread use in image synthesis and editing applications, their latent space is still not as well understood. Recently, a semantic latent space for DDMs, coined ‘h-space’, was shown to facilitate semantic image editing in a way reminiscent of GANs. The h-space is comprised of the bottleneck activations in the DDM’s denoiser across all timesteps of the diffusion process. In this paper, we explore the properties of h-space and propose several novel methods for finding meaningful semantic directions within it. We start by studying unsupervised methods for revealing interpretable semantic directions in pretrained DDMs. Specifically, we show that interpretable directions emerge as the principal components in the latent space. Additionally, we provide a novel method for discovering image-specific semantic directions by spectral analysis of the Jacobian of the denoiser w.r.t. the latent code. 
Next, we extend the analysis by finding directions in a supervised fashion in unconditional DDMs.
We demonstrate how such directions can be found by annotating generated samples with a domain-specific attribute classifier. 
We further show how to semantically disentangle the found directions by simple linear projection.
Our approaches are applicable without requiring any architectural modifications, text-based guidance, CLIP-based optimization, or model fine-tuning.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2023</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col-sm-2 preview">
      
        
          
          

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
    <img src="/assets/img/publication_preview/human_motion1.png" class="preview z-depth-1 rounded" width="100%" height="auto" alt="human_motion1.png" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">
  </picture>

  
</figure>

        
      
    </div>
  

  <!-- Entry bib key -->
  <div id="grashof_neural_2023" class="col-sm-8">
    <!-- Title -->
    <div class="title">Neural Network-Based Human Motion Predictor and Smoother</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Stella
            Graßhof</em>, Mathias
            Bastholm
          , and Sami S.
            Brandt
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>SN Computer Science</em>,  Sep 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Though continuous advances in the field of human pose estimation, it remains a challenge to retrieve high-quality recordings from real-life human motion using commodity hardware. Therefore, this work focuses on predicting and improving estimates for human motion with the aim of achieving production quality for skinned mesh animations by off-the-shelf webcams. We take advantage of recent findings in the field by employing a recurrent neural network architecture to (1) predict and (2) denoise human motion, with the intention of bridging the gap between cheap recording methods and high-quality recording. First, we propose an LSTM to predict short-term human motion, which achieves competitive results to state-of-the-art methods. Then, we adapt this model architecture and train it to clean up noisy human motion from two 3D low-quality input sources, and hence mimic a real-world scenario of recording human motion which yields noisy estimates. Experiments on simulated data show that the model is capable of significantly reducing noise, and it opens the way for future work to test the model on annotated data.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="ibh_tempose_2023" class="col-sm-8">
    <!-- Title -->
    <div class="title">TemPose: a new skeleton-based transformer model designed for fine-grained motion recognition in badminton</div>
    <!-- Author -->
    <div class="author">
      

      
      Magnus
            Ibh
          , <em>Stella
            Graßhof</em>, Dan
            Witzner
          , and
        <span class="more-authors" title="click to view 1 more author" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '1 more author' ? 'Pascal Madeleine' : '1 more author';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.text(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">1 more author</span>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em> ,  Jun 2023
    </div>
    <div class="periodical">
      ISSN: 2160-7516
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>This paper presents TemPose, a novel skeleton-based transformer model designed for fine-grained motion recognition to improve understanding of the detailed player actions in badminton. The model utilizes multiple temporal and interaction layers to capture variable-length multi-person human actions while minimizing reliance on non-human visual context. TemPose is evaluated on two fine-grained badminton datasets, where it significantly outperforms other baseline models by incorporating additional input streams, such as the shuttlecock position, into the temporal transformer layers of the model. Additionally, TemPose demonstrates great versatility by achieving competitive results compared to other state-of-the-art skeleton-based models on the large-scale action recognition benchmark NTU RGB+D. Experiments are conducted to explore how different model parameter configurations affect Tem-Pose’s performance. Additionally, a qualitative analysis of the temporal attention maps suggests that the model learns to prioritize frames of specific poses relevant to different actions while formulating an intuition of each individual’s importance in the sequences. Overall, TemPose is an intuitive and versatile architecture that has the potential to be further developed and incorporated into other methods for managing human motion in sports with state-of-the-art results.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="haas_controllable_2023" class="col-sm-8">
    <!-- Title -->
    <div class="title">Controllable GAN Synthesis Using Non-Rigid Structure-from-Motion</div>
    <!-- Author -->
    <div class="author">
      

      
      René
            Haas
          , <em>Stella
            Graßhof</em>, and Sami S.
            Brandt
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em> ,  Jun 2023
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>In this paper, we present an approach for combining non-rigid structure-from-motion (NRSfM) with deep generative models, and propose an efficient framework for discovering trajectories in the latent space of 2D GANs corresponding to changes in 3D geometry. Our approach uses recent advances in NRSfM and enables editing of the camera and non-rigid shape information associated with the latent codes without needing to retrain the generator. This formulation provides an implicit dense 3D reconstruction as it enables the image synthesis of novel shapes from arbitrary view angles and non-rigid structure. The method is built upon a sparse backbone, where a neural regressor is first trained to regress parameters describing the cameras and sparse non-rigid structure directly from the latent codes. The latent trajectories associated with changes in the camera and structure parameters are then identified by estimating the local inverse of the regressor in the neighborhood of a given latent code. The experiments show that our approach provides a versatile, systematic way to model, analyze, and edit the geometry and non-rigid structures of faces.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2022</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="bastholm_neural_2022" class="col-sm-8">
    <!-- Title -->
    <div class="title">Neural Network-based Human Motion Smoother</div>
    <!-- Author -->
    <div class="author">
      

      
      Mathias
            Bastholm
          , <em>Stella
            Graßhof</em>, and Sami
            Brandt
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em></em> Feb 2022
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Recording real life human motion as a skinned mesh animation with an acceptable quality is usually difficult. Even though recent advances in pose estimation have enabled motion capture from off-the-shelf webcams, the low quality makes it infeasible for use in production quality animation. This work proposes to use recent advances in the prediction of human motion through neural networks to augment low quality human motion, in an effort to bridge the gap between cheap recording methods and high quality recording. First, a model, competitive with prior work in short-term human motion prediction, is constructed. Then, the model is trained to clean up motion from two low quality input sources, mimicking a real world scenario of recording human motion through two webcams. Experiments on simulated data show that the model is capable of significantly reducing noise, and it opens the way for future work to test the model on annotated data.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="grashof_tensor-based_2022" class="col-sm-8">
    <!-- Title -->
    <div class="title">Tensor-Based Non-Rigid Structure from Motion</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Stella
            Graßhof</em>, and Sami Sebastian
            Brandt
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</em> ,  Jan 2022
    </div>
    <div class="periodical">
      ISSN: 2642-9381
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>In this work we present a method that combines tensor-based face modelling and analysis and non-rigid structure-from-motion (NRSFM). The core idea is to see that the conventional tensor formulation for the face structure and expression analysis can be utilised while the structure component can be directly analysed as the non-rigid structure-from-motion problem. To the NRSFM problem part we further present a novel prior-free approach that factorises the 2D input shapes into affine projection matrices, rank-one 3D affine basis shapes, and the basis shape coefficients. The linear combination of the basis shapes thus yields the recovered 3D shapes upto an affine transformation. In contrast to most works in literature, no calibration information of the cameras or structure prior is required. Experiments on challenging face datasets show that our method, with and without the metric upgrade, is accurate and fast when compared to the state-of-the-art and is well suitable for dense reconstruction and face editing.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="haas_tensor-based_2022" class="col-sm-8">
    <!-- Title -->
    <div class="title">Tensor-based Emotion Editing in the StyleGAN Latent Space</div>
    <!-- Author -->
    <div class="author">
      

      
      René
            Haas
          , <em>Stella
            Graßhof</em>, and Sami S.
            Brandt
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In </em> ,  May 2022
    </div>
    <div class="periodical">
      WS CVPR
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>In this paper, we use a tensor model based on the Higher-Order Singular Value Decomposition (HOSVD) to discover semantic directions in Generative Adversarial Networks. This is achieved by first embedding a structured facial expression database into the latent space using the e4e encoder. Specifically, we discover directions in latent space corresponding to the six prototypical emotions: anger, disgust, fear, happiness, sadness, and surprise, as well as a direction for yaw rotation. These latent space directions are employed to change the expression or yaw rotation of real face images. We compare our found directions to similar directions found by two other methods. The results show that the visual quality of the resultant edits are on par with State-of-the-Art. It can also be concluded that the tensor-based model is well suited for emotion and yaw editing, i.e., that the emotion or yaw rotation of a novel face image can be robustly changed without a significant effect on identity or other attributes in the images.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2021</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="grashof_multilinear_2021" class="col-sm-8">
    <!-- Title -->
    <div class="title">Multilinear Modelling of Faces and Expressions</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Stella
            Graßhof</em>, Hanno
            Ackermann
          , Sami Sebastian
            Brandt
          , and
        <span class="more-authors" title="click to view 1 more author" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '1 more author' ? 'Jörn Ostermann' : '1 more author';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.text(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">1 more author</span>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em>,  Oct 2021
    </div>
    <div class="periodical">
      IEEE Transactions on Pattern Analysis and Machine Intelligence
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>In this work, we present a new versatile 3D multilinear statistical face model, based on a tensor factorisation of 3D face scans, that decomposes the shapes into person and expression subspaces. Investigation of the expression subspace reveals an inherent low-dimensional substructure, and further, a star-shaped structure. This is due to two novel findings. (1) Increasing the strength of one emotion approximately forms a linear trajectory in the subspace. (2) All these trajectories intersect at a single point – not at the neutral expression as assumed by almost all prior works—but at an apathetic expression. We utilise these structural findings by reparameterising the expression subspace by the fourth-order moment tensor centred at the point of apathy. We propose a 3D face reconstruction method from single or multiple 2D projections by assuming an uncalibrated projective camera model. The non-linearity caused by the perspective projection can be neatly included into the model. The proposed algorithm separates person and expression subspaces convincingly, and enables flexible, natural modelling of expressions for a wide variety of human faces. Applying the method on independent faces showed that morphing between different persons and expressions can be performed without strong deformations.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="haas_tensor-based_2021" class="col-sm-8">
    <!-- Title -->
    <div class="title">Tensor-based Subspace Factorization for StyleGAN</div>
    <!-- Author -->
    <div class="author">
      

      
      René
            Haas
          , <em>Stella
            Graßhof</em>, and Sami S.
            Brandt
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em></em> Dec 2021
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>In this paper, we propose TGAN a tensor-based method for modeling the latent space of generative models. The objective is to identify semantic directions in latent space. To this end, we propose to fit a multilinear tensor model on a structured facial expression database, which is initially embedded into latent space. We validate our approach on StyleGAN trained on FFHQ using BU-3DFE as a structured facial expression database. We show how the parameters of the multilinear tensor model can be approximated by Alternating Least Squares. Further, we introduce a stacked style-separated tensor model, defined as an ensemble of style-specific models to integrate our approach with the extended latent space of StyleGAN. We show that taking the individual styles of the extended latent space into account leads to higher model flexibility and lower reconstruction error. Finally, we do several experiments comparing our approach to former work on both GANs and multilinear models. Concretely, we analyze the expression subspace and find that the expression trajectories meet at an apathetic face that is consistent with earlier work. We also show that by changing the pose of a person, the generated image from our approach is closer to the ground truth than results from two competing approaches.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2019</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="brandt_uncalibrated_2019" class="col-sm-8">
    <!-- Title -->
    <div class="title">Uncalibrated Non-Rigid Factorisation by Independent Subspace Analysis</div>
    <!-- Author -->
    <div class="author">
      

      
      Sami Sebastian
            Brandt
          , Hanno
            Ackermann
          , and <em>Stella
            Graßhof</em>
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)</em> ,  Oct 2019
    </div>
    <div class="periodical">
      ISSN: 2473-9944
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>We propose a general, prior-free approach for the uncalibrated non-rigid structure-from-motion problem for modelling and analysis of non-rigid objects such as human faces. We recover the non-rigid affine structure and motion from 2D point correspondences by assuming that (1) the non-rigid shapes are generated by a linear combination of rigid 3D basis shapes, (2) that the non-rigid shapes are affine in nature, i.e., they can be modelled as deviations from the mean, rigid shape, (3) and that the basis shapes are statistically independent. In contrast to the majority of existing works, no statistical prior is assumed for the structure and motion apart from the assumption that underlying basis shapes are statistically independent. The independent 3D shape bases are recovered by independent subspace analysis (ISA). Likewise, in contrast to the most previous approaches, no calibration information is assumed for affine cameras; the reconstruction is solved up to a global affine ambiguity that makes our approach simple and efficient. In the experiments, we evaluated the method with several standard data sets including a real face expression data set of 7200 faces with 2D point correspondences and unknown 3D structure and motion for which we obtained promising results.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="grashof_expressive_2019" class="col-sm-8">
    <!-- Title -->
    <div class="title">Expressive Personalized 3D Face Models from 3D Face Scans</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Stella
            Graßhof</em>
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      Oct 2019
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>In this work, different methods are presented to create 3D face models from databases of 3D face scans. The challenge in this endeavour is to balance the limited training data with the high demands of various applications. The 3D scans stem from various persons showing different expressions, with varying number of points per 3D scan […]</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2018</h2>
<ol class="bibliography"><li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="awiszus_unsupervised_2018" class="col-sm-8">
    <!-- Title -->
    <div class="title">Unsupervised Features for Facial Expression Intensity Estimation Over Time</div>
    <!-- Author -->
    <div class="author">
      

      
      M.
            Awiszus
          , S.
            Graßhof
          , F.
            Kuhnke
          , and
        <span class="more-authors" title="click to view 1 more author" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '1 more author' ? 'J. Ostermann' : '1 more author';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.text(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">1 more author</span>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</em> ,  Jun 2018
    </div>
    <div class="periodical">
      ISSN: 2160-7516
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>The diversity of facial shapes and motions among persons is one of the greatest challenges for automatic analysis of facial expressions. In this paper, we propose a feature describing expression intensity over time, while being invariant to person and the type of performed expression. Our feature is a weighted combination of the dynamics of multiple points adapted to the overall expression trajectory. We evaluate our method on several tasks all related to temporal analysis of facial expression. The proposed feature is compared to a state-of-the-art method for expression intensity estimation, which it outperforms. We use our proposed feature to temporally align multiple sequences of recorded 3D facial expressions. Furthermore, we show how our feature can be used to reveal person-specific differences in performances of facial expressions. Additionally, we apply our feature to identify the local changes in face video sequences based on action unit labels. For all the experiments our feature proves to be robust against noise and outliers, making it applicable to a variety of applications for analysis of facial movements.</p>
      </div>
    

    

    
  </div>
</div>
</li></ol>
<h2 class="bibliography">2017</h2>
<ol class="bibliography">
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="grashof_apathy_2017" class="col-sm-8">
    <!-- Title -->
    <div class="title">Apathy Is the Root of All Expressions</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Stella
            Graßhof</em>, Hanno
            Ackermann
          , Sami S.
            Brandt
          , and
        <span class="more-authors" title="click to view 1 more author" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '1 more author' ? 'Jörn Ostermann' : '1 more author';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.text(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">1 more author</span>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 2017 12th IEEE International Conference on Automatic Face Gesture Recognition (FG 2017)</em> ,  May 2017
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>In this paper, we present a new statistical model for human faces. Our approach is built upon a tensor factorisation model that allows controlled estimation, morphing and transfer of new facial shapes and expressions. We propose a direct parametrisation and regularisation for person and expression related terms so that the training database is well utilised. In contrast to existing works we are the first to reveal that the expression subspace is star shaped. This stems from the fact that increasing the strength of an expression approximately forms a linear trajectory in the expression subspace, and all these linear trajectories intersect in a single point which corresponds to the point of no expression or the point of apathy. After centring our analysis to this point, we then demonstrate how the dimensionality of the expression subspace can be further reduced by projection pursuit with the help of the fourth-order moment tensor. The results show that our method is able to achieve convincing separation of the person specific and expression subspaces as well as flexible, natural modelling of facial expressions for wide variety of human faces. By the proposed approach, one can morph between different persons and different expressions even if they do not exist in the database. In contrast to the state-of-the-art, the morphing works without causing strong deformations. In the application of expression classification, the results are also better.</p>
      </div>
    

    

    
  </div>
</div>
</li>
<li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="grashof_projective_2017" class="col-sm-8">
    <!-- Title -->
    <div class="title">Projective structure from facial motion</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Stella
            Graßhof</em>, Hanno
            Ackermann
          , Felix
            Kuhnke
          , and
        <span class="more-authors" title="click to view 2 more authors" onclick="
              var element = $(this);
              element.attr('title', '');
              var more_authors_text = element.text() == '2 more authors' ? 'Jörn Ostermann, Sami S. Brandt' : '2 more authors';
              var cursorPosition = 0;
              var textAdder = setInterval(function(){
                element.text(more_authors_text.substring(0, cursorPosition + 1));
                if (++cursorPosition == more_authors_text.length){
                  clearInterval(textAdder);
                }
            }, '10');
          ">2 more authors</span>
      
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 2017 Fifteenth IAPR International Conference on Machine Vision Applications (MVA)</em> ,  May 2017
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Nonrigid Structure-From-Motion is a well-known approach to estimate time-varying 3D structures from 2D input image sequences. For challenging problems such as the reconstruction of human faces, state-of-the-art approaches estimate statistical shape spaces from training data. It is common practice to use orthographic or weak-perspective camera models to map 3D to 2D points. We propose to use a projective camera model combined with a multilinear tensor-based face model, enabling approximation of a dense 3D face surface by sparse 2D landmarks. Using a projective camera is beneficial, as it is able to handle perspective projections and particular camera motions which are critical for affine models. We show how the nonlinearity of the projective model can be linearized so that its parameters can be estimated by an alternating-least-squares approach. This enables simple and fast estimation of the model parameters. The effectiveness of the proposed algorithm is demonstrated using challenging real image data.</p>
      </div>
    

    

    
  </div>
</div>
</li>
</ol>
<h2 class="bibliography">2015</h2>
<ol class="bibliography"><li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="grashof_estimation_2015" class="col-sm-8">
    <!-- Title -->
    <div class="title">Estimation of face parameters using correlation analysis and a topology preserving prior</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Stella
            Graßhof</em>, Hanno
            Ackermann
          , and Jörn
            Ostermann
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In 2015 14th IAPR International Conference on Machine Vision Applications (MVA)</em> ,  May 2015
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>Candide-3 is a well-known model, used to represent triangular meshes of human faces. It is common to only estimate 17 to 21 of the 79 model parameters. We show that these are insufficient to fit model vertices to facial feature points with low error and if more parameters are estimated, the model mesh deforms to unnatural configurations. To overcome this problem, we propose a novel solution: Given facial feature points, we propose to estimate the model parameters in subsets in which they are uncorrelated. Additionally we present a term to penalize topologically incorrect triangular mesh configurations. As a result the average mean squared error between facial feature points and model vertices is reduced by 90%, while face topology is preserved.</p>
      </div>
    

    

    
  </div>
</div>
</li></ol>
<h2 class="bibliography">2013</h2>
<ol class="bibliography"><li>
<div class="row">
  
    <div class="col-sm-2 abbr">
      
    </div>
  

  <!-- Entry bib key -->
  <div id="grashof_performance_2013" class="col-sm-8">
    <!-- Title -->
    <div class="title">Performance of Image Registration and Its Extensions for Interpolation of Facial Motion</div>
    <!-- Author -->
    <div class="author">
      

      
      <em>Stella
            Graßhof</em>, and J.
            Ostermann
          
    </div>

    <!-- Journal/Book title and date -->
    
    
    
    
    
    
    
    <div class="periodical">
      <em>In PSIVT Workshops</em> ,  May 2013
    </div>
    <div class="periodical">
      
    </div>

    <!-- Links/Buttons -->
    <div class="links">
      
        <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
      
      
      
      
      
      
      
      
      
      
      
      
    </div>
    
      
      
      
      
    

    
      <!-- Hidden abstract block -->
      <div class="abstract hidden">
        <p>We compare the performance of an intensity based nonparametric image registration algorithm and extensions applied to frame interpolation of mouth images. The mouth exhibits large deformations due to different shapes, additionally some facial features occlude others, e.g. the lips cover the teeth. The closures and disclosures represent a challenging problem, which cannot be solved by the traditional image registration algorithms. 
 
The tested extensions include local regularizer weight adaptation, incorporation of landmarks, self-occlusion handling and penalization of folds, which have all been examined with different weight parameters. 
 
Since the performance of these algorithms and extensions turns out to be superior in case of mouth closures, we provide an algorithm for the automatic selection of deformable template and static reference image for the registration procedure. Subjective tests show that the quality of results for interpolation of mouth images is enhanced by this proposal.</p>
      </div>
    

    

    
  </div>
</div>
</li></ol>

</div>

  </article>

  

  
</div>

      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      © Copyright 2024
      Stella
      
      Grasshof. 
      
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script>



<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script>
<script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script>
<script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>


  <script async src="https://badge.dimensions.ai/badge.js"></script>


    
  <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams',
      },
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


    


    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

  </body>
</html>
